import streamlit as st
import os
import re
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationChain

# OpenAI API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

st.markdown("<h1 style='text-align: center;'>ê³ ë¯¼ì— ë”°ë¥¸ ì œí’ˆ ì¶”ì²œ ì‹œìŠ¤í…œ</h1>", unsafe_allow_html=True)
st.markdown(
    "<h5 style='text-align: center; font-weight: 100'>ê³ ë¯¼ì„ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒë‹´ì„ ì§„í–‰í•˜ê³ , ì¶©ë¶„í•œ ëŒ€í™” ë§¥ë½ì´ í˜•ì„±ë˜ë©´ ì ì ˆí•œ ì¿ íŒ¡ ì œí’ˆì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.</h5>",
    unsafe_allow_html=True,
)

# ì¢Œìš° ì—¬ë°±ì„ ìœ„í•´ ì „ì²´ UIë¥¼ ì„¸ ì—´ë¡œ ë¶„í•  (ì™¼ìª½, ì¤‘ì•™, ì˜¤ë¥¸ìª½)
left_col, center_col, right_col = st.columns([2, 5, 2])

with center_col:
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¨¼ì € ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”."}
        ]
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(memory_key="history", return_messages=True)

    # OpenAI ëª¨ë¸ ì„¤ì •
    llm = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)

    # ì‚¬ìš©ì ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    custom_prompt = PromptTemplate(
        input_variables=["history", "input"],
        template="""
        ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì„¬ì„¸í•œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ê³ ë¯¼ì„ ì´ì•¼ê¸°í•˜ë©´ ë¨¼ì € ê³µê°í•˜ê³  ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ìƒë‹´ì„ ì§„í–‰í•˜ì‹­ì‹œì˜¤.
        ì¶©ë¶„í•œ ìƒë‹´ì´ ì´ë£¨ì–´ì¡Œë‹¤ê³  íŒë‹¨ë˜ë©´, ì ì ˆí•œ ì œí’ˆì´ ìˆì„ ê²½ìš° í•˜ë‚˜ë§Œ ì¶”ì²œí•˜ì‹­ì‹œì˜¤.
        ë‹¨, ë¬´ë¦¬í•˜ê²Œ ì œí’ˆì„ ì¶”ì²œí•˜ì§€ ë§ê³  ëŒ€í™” ë§¥ë½ì´ ì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨ë  ë•Œë§Œ ì¶”ì²œí•˜ì„¸ìš”.

        **ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ**:
        - ê³ ë¯¼ ìš”ì•½:
        - í•´ê²° ë°©ë²•:
        - ì¶”ì²œ ì œí’ˆ: (ì œí’ˆ ì¶”ì²œì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë  ê²½ìš°ë§Œ í‘œì‹œ)
        - ì¿ íŒ¡ ê²€ìƒ‰ í‚¤ì›Œë“œ: (ì œí’ˆ ì¶”ì²œì´ í•„ìš”í•  ê²½ìš°ë§Œ ì œê³µ)

        **ì…ë ¥ ì •ë³´**:
        - ì‚¬ìš©ì ê³ ë¯¼: {input}
        - ì´ì „ ëŒ€í™” ë‚´ìš©: {history}
        """
    )

    conversation_chain = ConversationChain(
        llm=llm,
        memory=st.session_state.memory,
        prompt=custom_prompt
    )

    # ê¸°ì¡´ ëŒ€í™” ë‚´ì—­ ì¶œë ¥ (ì±„íŒ… ë§í’ì„  ìŠ¤íƒ€ì¼)
    for message in st.session_state.chat_history:
        with st.chat_message("user" if message["role"] == "user" else "assistant"):
            st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° (ì…ë ¥ì°½ì€ st.chat_input() ì‚¬ìš©)
if prompt := st.chat_input("ğŸ’¬ ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    chat_history_data = st.session_state.memory.load_memory_variables({})
    history = chat_history_data.get("history", "")
    
    # GPT í˜¸ì¶œ: ëª¨ë¸ì´ ììœ¨ì ìœ¼ë¡œ ìƒë‹´ í›„ ì œí’ˆ ì¶”ì²œ ì—¬ë¶€ë¥¼ ê²°ì •
    response = conversation_chain.run({"history": history, "input": prompt})
    
    # ë§Œì•½ GPT ì‘ë‹µì— "ì¿ íŒ¡ ê²€ìƒ‰ í‚¤ì›Œë“œ:"ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì œí’ˆ ì¶”ì²œìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ë§í¬ ìƒì„±
    match = re.search(r"ì¿ íŒ¡ ê²€ìƒ‰ í‚¤ì›Œë“œ:\s*(.+)", response)
    if match:
        recommended_product = match.group(1).strip()
        coupang_link = f"https://www.coupang.com/np/search?q={recommended_product.replace(' ', '+')}"
        response += f"\n\nğŸ”— [ì¿ íŒ¡ì—ì„œ '{recommended_product}' ê²€ìƒ‰í•˜ê¸°]({coupang_link})"
    
    # GPT ì‘ë‹µì„ ëŒ€í™” ë‚´ì—­ì— ì¶”ê°€
    st.session_state.chat_history.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.write(response)
    
    st.rerun()