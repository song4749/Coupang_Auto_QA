import streamlit as st
import os
import re
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationChain

# OpenAI API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

st.markdown("<h1 style='text-align: center;'>ê³ ë¯¼ì— ë”°ë¥¸ ì œí’ˆ ì¶”ì²œ ì‹œìŠ¤í…œ</h1>", unsafe_allow_html=True)
st.markdown("<h5 style='text-align: center; font-weight: 100'>ê³ ë¯¼ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”! ë‹¹ì‹ ì˜ ìƒí™©ì„ ê³ ë ¤í•´ ì§€ê¸ˆ ê°€ì¥ í•„ìš”í•œ ì¿ íŒ¡ ì•„ì´í…œì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.<br><br><br></h5>", unsafe_allow_html=True)

# âœ… ë ˆì´ì•„ì›ƒ ì„¤ì • (ì™¼ìª½/ì˜¤ë¥¸ìª½ ê³µë°± ì¶”ê°€)
left_space, chat_area, right_space = st.columns([2, 5, 2])

with chat_area:
    # OpenAI ëª¨ë¸ ì„¤ì •
    llm = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)

    # âœ… ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ë©”ëª¨ë¦¬ ì¶”ê°€ (ê¸°ë³¸: ìµœê·¼ 10ê°œ ë©”ì‹œì§€ ì €ì¥)
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(memory_key="history", return_messages=True)

    # âœ… ì±„íŒ… ê¸°ë¡ ì €ì¥ ê³µê°„ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # âœ… ì‚¬ìš©ì ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€
    custom_prompt = PromptTemplate(
        input_variables=["history", "input"],
        template="""
        ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê³ ë¯¼ì„ ì´ì•¼ê¸°í•˜ë©´ ê³µê°í•´ì£¼ê³ , í˜„ì‹¤ì ì¸ í•´ê²°ì±…ì„ ì œì•ˆí•´ì•¼ í•©ë‹ˆë‹¤.
        ê³ ë¯¼ì„ ë“£ê³  ê·¸ ìƒí™©ì— í•„ìš”í•œ ì œí’ˆì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”. ì‚¬ìš©ìê°€ í•™ìƒì¸ì§€, ì§ì¥ì¸ì¸ì§€, ì–´ëŠì •ë„ì˜ ì˜ˆì‚°ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ê³ ë ¤í•˜ì„¸ìš”.

        - ì‚¬ìš©ì ê³ ë¯¼: {input}
        - ì´ì „ ëŒ€í™” ë‚´ìš©: {history}

        ğŸ’¡ ì¶œë ¥ í˜•ì‹:
        - ê³ ë¯¼ ìš”ì•½: 
        - í•´ê²° ë°©ë²•:
        - ì¶”ì²œ ì œí’ˆ: (í•œ ê°œë§Œ ì¶”ì²œ)
        - ì¿ íŒ¡ ê²€ìƒ‰ í‚¤ì›Œë“œ: (ì¶”ì²œ ì œí’ˆì˜ í•µì‹¬ í‚¤ì›Œë“œ, ì§§ê²Œ)
        """
    )

    # âœ… ëŒ€í™”í˜• ì²´ì¸ ìƒì„± (Retrieval ê¸°ëŠ¥ ì¶”ê°€ ê°€ëŠ¥)
    conversation_chain = ConversationChain(
        llm=llm,
        memory=st.session_state.memory,
        prompt=custom_prompt
    )

    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ (ì±„íŒ… ë§í’ì„  ìŠ¤íƒ€ì¼)
    for role, text in st.session_state.chat_history:
        with st.chat_message("user" if role == "ğŸ‘¤ ë‹¹ì‹ " else "assistant"):
            st.write(text)

# ê³ ì •ëœ ì…ë ¥ì°½ ì˜ì—­ ìƒì„±
user_input = st.chat_input("ğŸ’¬ ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”...")

# âœ… ì‘ë‹µ ì²˜ë¦¬
if user_input:
    # âœ… chat_history ì˜ˆì™¸ ì²˜ë¦¬ (ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)
    chat_history_data = st.session_state.memory.load_memory_variables({})
    history = chat_history_data.get("history", "")

    # âœ… GPTì— ìš”ì²­í•˜ì—¬ ì œí’ˆ ì¶”ì²œ ë°›ê¸°
    response = conversation_chain.run({"history": history, "input": user_input})

    # âœ… ì¶”ì²œ ì œí’ˆ ì¶”ì¶œ (GPTê°€ ìƒì„±í•œ ì‘ë‹µì—ì„œ ì œí’ˆëª… ì°¾ê¸° - ì •ê·œì‹ ì‚¬ìš©)
    match = re.search(r"ì¿ íŒ¡ ê²€ìƒ‰ í‚¤ì›Œë“œ:\s*(.+)", response)
    recommended_product = match.group(1).strip() if match else None

    # âœ… ì¿ íŒ¡ ê²€ìƒ‰ ë§í¬ ìƒì„± (GPTê°€ ì¶”ì²œí•œ ì œí’ˆìœ¼ë¡œ)
    if recommended_product:
        coupang_link = f"https://www.coupang.com/np/search?q={recommended_product.replace(' ', '+')}"
        response += f"\n\nğŸ”— [ì¿ íŒ¡ì—ì„œ '{recommended_product}' ê²€ìƒ‰í•˜ê¸°]({coupang_link})"

    # âœ… ì±„íŒ… ê¸°ë¡ ì €ì¥
    st.session_state.chat_history.append(("ğŸ‘¤ ë‹¹ì‹ ", user_input))
    st.session_state.chat_history.append(("ğŸ¤– ì±—ë´‡", response))

    # âœ… ìƒˆ ë©”ì‹œì§€ ë°”ë¡œ í‘œì‹œ (ì…ë ¥ í›„ ìë™ ìŠ¤í¬ë¡¤)
    st.rerun()