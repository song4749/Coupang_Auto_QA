import streamlit as st
import subprocess
import shutil
import os
from langchain_community.document_loaders import BSHTMLLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def get_api_key():
    # í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key is None:
        st.error("ğŸš¨ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


# âœ… í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜
def run_crawler(link):
    """ğŸ“Œ `jpg_crowling.py` ì‹¤í–‰ (ìƒí’ˆ ë§í¬ì—ì„œ ì´ë¯¸ì§€ í¬ë¡¤ë§)"""
    try:
        subprocess.run(["python", "jpg_crowling.py", link], check=True)

    except Exception as e:
        st.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ ë°œìƒ: {e}")


# âœ… OCR ì‹¤í–‰ í•¨ìˆ˜
def run_ocr():
    """ğŸ“Œ `jpg2text.ipynb` ì‹¤í–‰ (ì´ë¯¸ì§€ â†’ HTML ë³€í™˜)"""
    try:
        subprocess.run(["python", "jpg2text_run.py"], check=True)

    except Exception as e:
        st.error(f"âŒ OCR ì˜¤ë¥˜ ë°œìƒ: {e}")


@st.cache_resource
def load_vector_store():
    vectorstore = None  

    # âœ… HTML í´ë” ë‚´ ëª¨ë“  íŒŒì¼ì„ ë‹¤ì‹œ ë²¡í„° DBë¡œ ì €ì¥
    for filename in os.listdir(html_folder_path):
        if filename.endswith(".html"):
            file_path = os.path.join(html_folder_path, filename)
            try:
                # âœ… ê° HTML íŒŒì¼ì„ ê°œë³„ ë¬¸ì„œë¡œ ë¡œë“œ
                loader = BSHTMLLoader(file_path, open_encoding="utf-8", bs_kwargs={"features": "html.parser"})
                documents = loader.load()

                # âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì„œ)
                if vectorstore is None:
                    vectorstore = FAISS.from_documents(documents, embeddings)
                else:
                    vectorstore.add_documents(documents)  # ê¸°ì¡´ DBì— ì¶”ê°€

                print(f"âœ… {filename} ì²˜ë¦¬ ì™„ë£Œ! ({len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨)")

                # âœ… HTML íŒŒì¼ ì‚­ì œ
                os.remove(file_path)
                print(f"ğŸ—‘ï¸ {filename} ì‚­ì œ ì™„ë£Œ!")

            except Exception as e:
                print(f"âŒ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue  

    # âœ… ìƒˆë¡œìš´ ë²¡í„° DB ì €ì¥
    if vectorstore:
        vectorstore.save_local("faiss_index")
        print("âœ… ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
        return vectorstore
    else:
        print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨. HTML íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None
        

# âœ… ë²¡í„° DB ì‚­ì œ í•¨ìˆ˜
def delete_vector_db():
    """ì„¸ì…˜ì´ ì¢…ë£Œë  ë•Œ ë²¡í„° DB ì‚­ì œ"""
    if os.path.exists("faiss_index"):
        shutil.rmtree("faiss_index")
        print("ğŸ—‘ ì„¸ì…˜ ì¢…ë£Œ ê°ì§€ - ë²¡í„° DB ì‚­ì œ ì™„ë£Œ!")


# âœ… Streamlit ì„¸ì…˜ ìƒíƒœ í™•ì¸ ë° ë²¡í„° DB ì‚­ì œ ë¡œì§ ì ìš©
if "session_active" not in st.session_state:
    # ğŸš€ ì„¸ì…˜ì´ ìƒˆë¡œ ì‹œì‘ë¨ (ì¦‰, ìƒˆë¡œê³ ì¹¨ ë˜ëŠ” í˜ì´ì§€ ë‹«ê¸° í›„ ë‹¤ì‹œ ì ‘ì†í•œ ê²½ìš°)
    st.cache_resource.clear()
    delete_vector_db()  # âœ… ë²¡í„° DB ì‚­ì œ ì‹¤í–‰
    

# Streamlit UI
st.title("ì¿ íŒ¡ ìë™ì‘ë‹µ ì‹œìŠ¤í…œ")
st.write("ì¿ íŒ¡ ìƒí’ˆ ë§í¬ì™€ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì‹œë©´ ìë™ìœ¼ë¡œ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤!")

link = st.text_area("ğŸ”— ìƒí’ˆ íŒë§¤ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="https://www.coupang.com/vp/products/123456...")

if st.button("ğŸ–¼ ì´ë¯¸ì§€ í¬ë¡¤ë§ ì‹¤í–‰"):
    if link:
        with st.spinner("ğŸ”„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            run_crawler(link)
        st.success("âœ… ì´ë¯¸ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")

        with st.spinner("ğŸ”„ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘..."):
            run_ocr()
        st.success("âœ… ë³€í™˜ ì™„ë£Œ! ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # âœ… ë²¡í„° DBê°€ í•„ìš”í•  ê²½ìš° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.data_ready = True

    else:
        st.error("âŒ ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”!")

if "data_ready" not in st.session_state:
    st.stop()  # ğŸš€ ì‚¬ìš©ìê°€ ë§í¬ ì…ë ¥ í›„ ì‹¤í–‰ë˜ë„ë¡ ì¤‘ë‹¨

get_api_key()

# âœ… HTML íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ
html_folder_path = "ocr_texts"  # ì—¬ëŸ¬ ê°œì˜ HTML íŒŒì¼ì´ ìˆëŠ” í´ë”

# âœ… OpenAI Embeddings ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ìºì‹± ì ìš©)
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = load_vector_store()
vectorstore = st.session_state.vectorstore

# âœ… OpenAI LLM (GPT-4 Turbo) ì„¤ì •
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.3)

# âœ… ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ Retriever ì„¤ì •
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # ê°€ì¥ ê´€ë ¨ ìˆëŠ” 3ê°œ ë¬¸ì„œ ê²€ìƒ‰

# âœ… Prompt í…œí”Œë¦¿ ì„¤ì • (ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬í•¨í•œ ì§ˆì˜ ì‘ë‹µ)
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    ë‹¹ì‹ ì€ ê³ ê° ì§€ì› ì±—ë´‡ì…ë‹ˆë‹¤.  
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.  
    
    âœ… **ë‹µë³€ ë°©ì‹**  
    - ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìœ¼ë©´, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì‰½ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…**í•˜ì„¸ìš”.  
    - ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ, **í•„ìš”í•˜ë©´ ì¶”ê°€ ì„¤ëª…ì„ ë§ë¶™ì´ì„¸ìš”**.  
    - ë„ˆë¬´ ì§§ê±°ë‚˜ ë”±ë”±í•œ ë‹µë³€ ëŒ€ì‹ , **ì¹œì ˆí•˜ê³  ë¶€ë“œëŸ¬ìš´ í†¤ìœ¼ë¡œ ì‘ë‹µ**í•˜ì„¸ìš”.  

    âŒ **ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì§€ ëª»í•œ ê²½ìš°**  
    - "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  
    ë³´ë‹¤ ìì„¸í•œ ì‚¬í•­ì€ íŒë§¤ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”." ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.  

    ğŸŒŸ **ì¶”ê°€ ì‚¬í•­**  
    - ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¼ë„ **ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì• ë§¤í•˜ë©´**, í™•ì‹¤í•œ ë¶€ë¶„ë§Œ ë‹µë³€í•˜ì„¸ìš”.  
    - íŒë§¤ì ë¬¸ì˜ë¥¼ ìœ ë„í•  ë•Œ, **ì—°ë½ì²˜ ì •ë³´ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì œê³µ**í•˜ì„¸ìš”.

    ë¬¸ì„œ ë‚´ìš©:
    {context}

    ì‚¬ìš©ì ì§ˆë¬¸:
    {question}

    ë‹µë³€:
    """,
)

# âœ… RAG ê¸°ë°˜ QA ì‹œìŠ¤í…œ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False,  # ì°¸ê³ í•œ ë¬¸ì„œë„ í•¨ê»˜ ë°˜í™˜
    chain_type_kwargs={"prompt": prompt_template}
)

user_input = st.text_area("âœï¸ í•´ë‹¹ ìƒí’ˆì— ê´€í•˜ì—¬ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë´ ì£¼ì„¸ìš”", placeholder="ex)ë°°ì†¡ì´ ì–¼ë§ˆë‚˜ ê±¸ë ¤?")

if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if user_input:
        with st.spinner("ğŸ”„ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘..."):
            response = qa_chain.invoke({"query": user_input})
            answer = response.get("result")
        st.markdown(f"ğŸ“Œ ë‹µë³€ ê²°ê³¼: \n\n{answer}")
    
    else:
        st.error("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")