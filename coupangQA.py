import streamlit as st
import os
from langchain_community.document_loaders import BSHTMLLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def get_api_key():
    # í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key is None:
        st.error("ğŸš¨ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


@st.cache_resource
def load_vector_store():
    """ğŸ“Œ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±"""
    if os.path.exists("faiss_index"):
        return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    else:
        vectorstore = None  
        for filename in os.listdir(html_folder_path):
            if filename.endswith(".html"):
                file_path = os.path.join(html_folder_path, filename)
                try:
                    # âœ… ê° HTML íŒŒì¼ì„ ê°œë³„ ë¬¸ì„œë¡œ ë¡œë“œ
                    loader = BSHTMLLoader(file_path, open_encoding="utf-8", bs_kwargs={"features": "html.parser"})
                    documents = loader.load()
                    
                    # âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
                    if vectorstore is None:
                        vectorstore = FAISS.from_documents(documents, embeddings)
                    else:
                        vectorstore.add_documents(documents)

                    print(f"âœ… {filename} ì²˜ë¦¬ ì™„ë£Œ! ({len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨)")
                except Exception as e:
                    print(f"âŒ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue  

        # âœ… ë²¡í„° DB ì €ì¥
        if vectorstore:
            vectorstore.save_local("faiss_index")
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            return vectorstore
        else:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨. HTML íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            return None
        

# Streamlit UI
st.title("ì¿ íŒ¡ ìë™ì‘ë‹µ ì‹œìŠ¤í…œ")
st.write("ì¿ íŒ¡ ìƒí’ˆ ë§í¬ì™€ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì‹œë©´ ìë™ìœ¼ë¡œ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤!")

user_input = st.text_area("âœï¸ í•´ë‹¹ ìƒí’ˆì— ê´€í•˜ì—¬ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë´ ì£¼ì„¸ìš”", "")

get_api_key()

# âœ… HTML íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ
html_folder_path = "ocr_texts"  # ì—¬ëŸ¬ ê°œì˜ HTML íŒŒì¼ì´ ìˆëŠ” í´ë”

# âœ… OpenAI Embeddings ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ìºì‹± ì ìš©)
vectorstore = load_vector_store()

# âœ… OpenAI LLM (GPT-4 Turbo) ì„¤ì •
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.3)

# âœ… ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ Retriever ì„¤ì •
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # ê°€ì¥ ê´€ë ¨ ìˆëŠ” 3ê°œ ë¬¸ì„œ ê²€ìƒ‰

# âœ… Prompt í…œí”Œë¦¿ ì„¤ì • (ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬í•¨í•œ ì§ˆì˜ ì‘ë‹µ)
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    ë‹¹ì‹ ì€ ê³ ê° ì§€ì› ì±—ë´‡ì…ë‹ˆë‹¤.  
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.  
    
    âœ… **ë‹µë³€ ë°©ì‹**  
    - ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìœ¼ë©´, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì‰½ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…**í•˜ì„¸ìš”.  
    - ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ, **í•„ìš”í•˜ë©´ ì¶”ê°€ ì„¤ëª…ì„ ë§ë¶™ì´ì„¸ìš”**.  
    - ë„ˆë¬´ ì§§ê±°ë‚˜ ë”±ë”±í•œ ë‹µë³€ ëŒ€ì‹ , **ì¹œì ˆí•˜ê³  ë¶€ë“œëŸ¬ìš´ í†¤ìœ¼ë¡œ ì‘ë‹µ**í•˜ì„¸ìš”.  

    âŒ **ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì§€ ëª»í•œ ê²½ìš°**  
    - "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  
    ë³´ë‹¤ ìì„¸í•œ ì‚¬í•­ì€ íŒë§¤ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”." ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.  

    ğŸŒŸ **ì¶”ê°€ ì‚¬í•­**  
    - ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¼ë„ **ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì• ë§¤í•˜ë©´**, í™•ì‹¤í•œ ë¶€ë¶„ë§Œ ë‹µë³€í•˜ì„¸ìš”.  
    - íŒë§¤ì ë¬¸ì˜ë¥¼ ìœ ë„í•  ë•Œ, **ì—°ë½ì²˜ ì •ë³´ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì œê³µ**í•˜ì„¸ìš”.

    ë¬¸ì„œ ë‚´ìš©:
    {context}

    ì‚¬ìš©ì ì§ˆë¬¸:
    {question}

    ë‹µë³€:
    """,
)

# âœ… RAG ê¸°ë°˜ QA ì‹œìŠ¤í…œ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False,  # ì°¸ê³ í•œ ë¬¸ì„œë„ í•¨ê»˜ ë°˜í™˜
    chain_type_kwargs={"prompt": prompt_template}
)

# âœ… ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ì˜ˆì œ)
if st.button("ì§ˆë¬¸í•˜ê¸°"):
    response = qa_chain.invoke({"query": user_input})
    answer = response.get("result")
    st.markdown(f"ğŸ“Œ ë‹µë³€ ê²°ê³¼: \n\n{answer}")