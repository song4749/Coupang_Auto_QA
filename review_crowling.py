import sys
import asyncio
import pandas as pd
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# âœ… Windows í™˜ê²½ì—ì„œ UTF-8ë¡œ ì¶œë ¥ë˜ë„ë¡ ì„¤ì •
sys.stdout.reconfigure(encoding="utf-8")

star_folder = "review\star"
summery_folder = "review\summery"
best_review_folder = "review\best"

# âœ… í¬ë¡¤ë§í•  ì œí’ˆ URL (ì¿ íŒ¡ ì œí’ˆ ë¦¬ë·° í˜ì´ì§€ URL ì…ë ¥)
PRODUCT_URL = "https://www.coupang.com/vp/products/123456789?itemId=987654321"

# âœ… í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
MAX_PAGES = 5

def get_page_source(page, url):
    """ âœ… HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    page.goto(url)
    page.wait_for_selector(".sdp-review__article__list__review")  # âœ… ë¦¬ë·°ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    return page.content()  # âœ… HTML ì†ŒìŠ¤ ë°˜í™˜

def extract_reviews_from_html(html):
    """ âœ… HTMLì—ì„œ ë¦¬ë·° ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ """
    soup = BeautifulSoup(html, "html.parser")
    review_elements = soup.select(".sdp-review__article__list__review__content")

    reviews = [review.get_text(strip=True) for review in review_elements]
    return reviews

def scrape_coupang_reviews():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)  # âœ… Headless ëª¨ë“œ ì‹¤í–‰ (UI ì—†ì´ ì‹¤í–‰)
        page = browser.new_page()

        all_reviews = []
        current_page = 1

        while current_page <= MAX_PAGES:
            print(f"ğŸ”¹ {current_page}í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

            # âœ… 1ï¸âƒ£ HTML ì†ŒìŠ¤ë¥¼ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
            html = get_page_source(page, PRODUCT_URL)

            # âœ… 2ï¸âƒ£ HTMLì—ì„œ ë¦¬ë·° ì¶”ì¶œ (ë™ê¸° ë°©ì‹)
            reviews = extract_reviews_from_html(html)
            all_reviews.extend(reviews)

            print(f"âœ… {current_page}í˜ì´ì§€ì—ì„œ {len(reviews)}ê°œì˜ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")

            # âœ… 3ï¸âƒ£ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            try:
                next_button = page.query_selector(".sdp-review__article__page__next")
                if next_button:
                    next_button.click()
                    time.sleep(3)  # âœ… í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë¹„ë™ê¸° ëŒ€ë¹„ ì•ˆì •ì )
                    current_page += 1
                else:
                    print("ğŸš« ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì—†ìŒ. ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.")
                    break
            except Exception as e:
                print("ğŸš« ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

        browser.close()

        # âœ… í¬ë¡¤ë§ ì™„ë£Œ í›„ ë°ì´í„° ì €ì¥
        df = pd.DataFrame(all_reviews, columns=["ë¦¬ë·° ë‚´ìš©"])
        df.to_csv("coupang_reviews_sync.csv", index=False, encoding="utf-8-sig")
        print(f"âœ… ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: coupang_reviews_sync.csv")

# âœ… ì‹¤í–‰
scrape_coupang_reviews()