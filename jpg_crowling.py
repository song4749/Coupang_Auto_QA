import sys
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import os
import re
import shutil
import requests
from PIL import Image
from io import BytesIO

# âœ… Windows í™˜ê²½ì—ì„œ UTF-8ë¡œ ì¶œë ¥ë˜ë„ë¡ ì„¤ì •
sys.stdout.reconfigure(encoding="utf-8")

save_folder = "download_images"
main_image_folder = "main_image"
html_folder = "ocr_texts"
if not os.path.exists(save_folder):
    os.makedirs(save_folder)


def get_html(url):
    """Playwrightë¥¼ ì‚¬ìš©í•´ HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, args=["--no-sandbox", "--disable-gpu"])  # ë¸Œë¼ìš°ì € ë³´ì´ê²Œ ì‹¤í–‰ (ë””ë²„ê¹… ê°€ëŠ¥)
        context = browser.new_context()
        page = context.new_page()

        headers = {
            'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36"
        }
        page.set_extra_http_headers(headers)

        # âœ… ëœë¤í•œ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€ (1.5 ~ 5ì´ˆ)
        # time.sleep(random.uniform(1.5, 5.0))

        try:
            # í˜ì´ì§€ ì´ë™ (HTMLë§Œ ë¡œë“œë˜ë©´ ê°€ì ¸ì˜¤ê¸°)
            page.goto(url, timeout=60000, wait_until="load")

            if page.wait_for_selector("div.subType-IMAGE img, div.subType-TEXT img", timeout=20000):

                # âœ… JavaScript ì‹¤í–‰ í›„ ë™ì ìœ¼ë¡œ ìƒì„±ëœ HTML ê°€ì ¸ì˜¤ê¸°
                html = page.evaluate("document.documentElement.outerHTML")
                browser.close()
                return html, True
            else:
                browser.close()
                return None, False
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            browser.close()
            return None, False


def extract_filtered_images(html):
    """HTMLì—ì„œ íŠ¹ì • í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ì´ë¯¸ì§€ URLë§Œ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, "html.parser")

    # 'subType-IMAGE','subType-TEXT' ì°¾ê¸°
    image_containers = soup.select("div.subType-IMAGE, div.subType-TEXT")

    image_urls = []

    # ğŸ”¹ ê°€ì ¸ì˜¬ ì´ë¯¸ì§€ í™•ì¥ì ëª©ë¡ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬)
    valid_extensions = {"jpg", "jpeg", "png", "gif", "bmp", "webp", "svg", "tiff"}

    for container in image_containers:
        # í•´ë‹¹ div ë‚´ì˜ ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
        img_tags = container.find_all("img")

        for img in img_tags:
            img_url = img.get("src") or img.get("data-src")  # srcê°€ ì—†ìœ¼ë©´ data-src ì²´í¬

            if img_url:
                # ğŸ”¹ URLì—ì„œ í™•ì¥ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ í•„í„°ë§
                ext = img_url.split(".")[-1].split("?")[0].lower()
                if ext in valid_extensions:
                    # ìƒëŒ€ URLì´ë©´ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if img_url.startswith("//"):
                        img_url = "https:" + img_url
                    image_urls.append(img_url)

    return image_urls


def download_images(image_urls):
    """ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥"""
    for i, img_url in enumerate(image_urls, 1):
        # ì €ì¥ ê²½ë¡œ ì„¤ì • (ì´ë¯¸ì§€ í™•ì¥ì ìœ ì§€)
        ext = img_url.split(".")[-1].split("?")[0]  # í™•ì¥ì ì¶”ì¶œ (jpg, png ë“±, URLì— ? ë¶™ì–´ ìˆëŠ” ê²½ìš° ì œê±°)
        if ext.lower() not in ["jpg", "jpeg", "png"]:  # í™•ì¥ìê°€ ì´ìƒí•˜ë©´ ê¸°ë³¸ jpg ì‚¬ìš©
            ext = "jpg"
        save_path = os.path.join(save_folder, f"image_{i}.{ext}")

        try:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = requests.get(img_url, stream=True)
            if response.status_code == 200:
                image = Image.open(BytesIO(response.content))

                # âœ… RGBA ë˜ëŠ” P ëª¨ë“œ ì´ë¯¸ì§€ëŠ” RGBë¡œ ë³€í™˜ í›„ ì €ì¥
                if image.mode in ("RGBA", "P"):
                    image = image.convert("RGB")

                image.save(save_path)  # ë³€í™˜ëœ ì´ë¯¸ì§€ ì €ì¥
                print(f"âœ… {i}. ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path}")
            else:
                print(f"âŒ {i}. ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_url}")
        except Exception as e:
            print(f"âŒ {i}. ì˜¤ë¥˜ ë°œìƒ: {e}")


def product_image_and_name_download(html):
    soup = BeautifulSoup(html, "html.parser")
    img_tag = soup.find("img", class_="prod-image__detail")

    if img_tag:
        img_url = "https:" + img_tag["src"]  # src ê°’ì´ //ë¡œ ì‹œì‘í•˜ë¯€ë¡œ https:ë¥¼ ë¶™ì—¬ì•¼ í•¨

        img_response = requests.get(img_url)

        if img_response.status_code == 200:

            # í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
            if not os.path.exists(main_image_folder):
                os.makedirs(main_image_folder)

            image_path = os.path.join(main_image_folder, "main_image.jpg")

            with open(image_path, "wb") as f:
                f.write(img_response.content)
            print("ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: main_image.jpg")
        else:
            print("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
    else:
        print("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

    name = soup.find("h1", class_="prod-buy-header__title").text.strip()

    name_path = os.path.join(main_image_folder, "product_name.txt")
    if name:
        with open(name_path, "w", encoding="utf-8") as file:
            file.write(name)

    price_div = soup.find("div", class_="prod-price-onetime")
    if price_div:
        price_html = price_div.prettify()  # HTMLì„ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
        price_html = re.sub(r'\n\s*\n+', '\n', price_html)  # ì—¬ëŸ¬ ê°œì˜ ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°
        price_html = re.sub(r'>\s+<', '><', price_html)  # íƒœê·¸ ì‚¬ì´ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°

    price_path = os.path.join(html_folder, "price_info.html")
    with open(price_path, "w", encoding="utf-8") as file:
        file.write(price_html)


def basic_information(html):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="prod-delivery-return-policy-table essential-info-table")

    if table:
        if not os.path.exists(html_folder):
            os.makedirs(html_folder)
        
        file_path = os.path.join(html_folder, "basic_data.html")

        # í…Œì´ë¸” HTML ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(str(table))

        print(f"í…Œì´ë¸” HTML ì €ì¥ ì™„ë£Œ: {file_path}")
    else:
        print("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")


def delibery_data(html):
    soup = BeautifulSoup(html, "html.parser")
    li_element = soup.find_all("li", class_="product-etc tab-contents__content etc-new-style")

    if li_element:
        file_path = os.path.join(html_folder, "li_data.html")

        # ëª¨ë“  <li> íƒœê·¸ë¥¼ í•˜ë‚˜ì˜ HTML íŒŒì¼ì— ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            for li in li_element:
                f.write(str(li) + "\n")  # HTML ê·¸ëŒ€ë¡œ ì €ì¥ + ì¤„ë°”ê¿ˆ ì¶”ê°€

        print(f"<li> HTML ì €ì¥ ì™„ë£Œ: {file_path}")
    else:
        print("<li> íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")


# âœ… ëª…ë ¹ì¤„ ì¸ìë¡œ URLì„ ë°›ê¸°
# if len(sys.argv) < 2:
#     print("âŒ ì‚¬ìš©ë²•: python jpg_crowling.py <ì¿ íŒ¡ ìƒí’ˆ URL>")
#     sys.exit(1)

# url = sys.argv[1]  # âœ… ëª…ë ¹ì¤„ì—ì„œ URL ë°›ê¸°

url = "https://www.coupang.com/vp/products/8338421081?itemId=24078900518&vendorItemId=83384767739&q=%EB%83%89%EC%9E%A5%EA%B3%A0&itemsCount=27&searchId=31fcffc05584302&rank=0&searchRank=0&isAddedCart="

# âœ… ì¿ íŒ¡ ì œí’ˆ URL
html_source, S_or_F = get_html(url)

# âœ… íŠ¹ì • í´ë˜ìŠ¤ ì•ˆì— ìˆëŠ” jpg, png ì´ë¯¸ì§€ URL ì¶”ì¶œ
filtered_image_urls = extract_filtered_images(html_source)

# âœ… ê²°ê³¼ ì¶œë ¥
print("ì´ ì´ë¯¸ì§€ ê°œìˆ˜:", len(filtered_image_urls))

# âœ… ì´ë¯¸ì§€ ì‚­ì œ(ìˆë‹¤ë©´) í›„ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
if S_or_F:
    folders_to_clear = ["download_images", "main_image", "ocr_texts"]

    for folder in folders_to_clear:
        if os.path.exists(folder):  # âœ… í´ë” ì¡´ì¬ í™•ì¸
            for item in os.listdir(folder):  # âœ… í´ë” ë‚´ë¶€ íŒŒì¼ ë° í´ë” ìˆœíšŒ
                item_path = os.path.join(folder, item)
                
                if os.path.isfile(item_path):  # âœ… íŒŒì¼ì´ë©´ ì‚­ì œ
                    os.remove(item_path)
                elif os.path.isdir(item_path):  # âœ… í´ë”ì´ë©´ í´ë” ì‚­ì œ (í•˜ìœ„ íŒŒì¼ í¬í•¨)
                    shutil.rmtree(item_path)

    download_images(filtered_image_urls)

# ë©”ì¸ ì´ë¯¸ì§€, í•„ìˆ˜ í‘œê¸°ì •ë³´, ë°°ì†¡/êµí™˜/ë°˜í’ˆ ì•ˆë‚´ ë‹¤ìš´ë¡œë“œ
product_image_and_name_download(html_source)

basic_information(html_source)

delibery_data(html_source)